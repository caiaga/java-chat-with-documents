server.port=${PORT:8080}
logging.level.org.atmosphere=warn
spring.mustache.check-template-location=false
# Launch the default browser when starting the application in development mode
vaadin.launch-browser=true
vaadin.allowed-packages=com.vaadin,org.vaadin,dev.hilla,com.vaadin.demo
spring.jpa.defer-datasource-initialization=true
spring.sql.init.mode=always

### DocsChat config

# Streaming Chat
# - NOTE: Use ONLY ONE of the two options below at a time.
# - NOTE: Values starting with YOUR_** are placeholders and should be replaced with actual values.

# OpenAI API
# Better quality, requires sending data to OpenAI
langchain4j.open-ai.streaming-chat-model.api-key=${OPENAI_API_KEY}
langchain4j.open-ai.streaming-chat-model.model-name=gpt-4o

# Local OpenAI compatible API (ollama)
# Not as performant, but your data does not leave your computer
langchain4j.open-ai.streaming-chat-model.api-key=ollama
langchain4j.open-ai.streaming-chat-model.base-url=http://localhost:11434/v1
langchain4j.open-ai.streaming-chat-model.model-name=llama3.1:latest


# Embeddings
# The embedding model to use - default or openai
ai.embedding-model=default

# In case of using OpenAI - embeddings config
open-ai.embedding-model.api-key=${OPENAI_API_KEY}

# The embedding store to use - pinecone or inmemory
ai.embedding-store=inmemory
pinecone.api-key=${PINECONE_API_KEY}
# use openai1536 or local384
pinecone.index=local384

# Documentation source type to use - github or local
ai.docs.source.type=local

# Filesystem path to your documentation folder
ai.docs.location=${LOCAL_DOC_DIRECTORY}

# Github configuration
github.access.token=${GITHUB_ACCESS_TOKEN}
github.repo=hclds-keycloak
github.branch=main
github.owner=HCL-TECH-SOFTWARE


# Debug logging to print requests
logging.level.dev.langchain4j=DEBUG
logging.level.dev.ai4j.openai4j=DEBUG
langchain4j.open-ai.streaming-chat-model.log-requests=true
langchain4j.open-ai.streaming-chat-model.log-responses=true