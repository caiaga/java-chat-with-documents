server.port=${PORT:8080}
logging.level.org.atmosphere=warn
spring.mustache.check-template-location=false
# Launch the default browser when starting the application in development mode
vaadin.launch-browser=true
vaadin.allowed-packages=com.vaadin,org.vaadin,dev.hilla,com.vaadin.demo
spring.jpa.defer-datasource-initialization=true
spring.sql.init.mode=always

### DocsChat config

# Streaming Chat
# - NOTE: Use ONLY ONE of the two options below at a time.

# OpenAI API
# Better quality, requires sending data to OpenAI
langchain4j.open-ai.streaming-chat-model.api-key=${OPENAI_API_KEY}
langchain4j.open-ai.streaming-chat-model.model-name=${CHAT_MODEL:gpt-4o}

# Local OpenAI compatible API (ollama)
# Not as performant, but your data does not leave your computer
#langchain4j.open-ai.streaming-chat-model.api-key=ollama
#langchain4j.open-ai.streaming-chat-model.base-url=${OLLAMA_BASEURL:http://localhost:11434/v1}
#langchain4j.open-ai.streaming-chat-model.model-name=llama3.1:latest


# Embeddings
# The embedding model to use - default or openai
ai.embedding-model=${EMDEDDING-MODEL:default}
ai.injest.batch.size=${BATCH_SIZE:10}

# In case of using OpenAI - embeddings config
open-ai.embedding-model.api-key=${OPENAI_API_KEY}

# The embedding store to use - pinecone or inmemory
ai.embedding-store=${EMBEDDING-STORE:inmemory}
pinecone.api-key=${PINECONE_API_KEY}
# use openai1536 or local384
pinecone.index=${PINECONE_INDEX:local384}

# Documentation source type to use - github or local
ai.docs.source.type=${DOC_SOURCE:local}

# Filesystem path to your documentation folder
ai.docs.location=${LOCAL_DOC_DIRECTORY}

# Github configuration
github.access.token=${GITHUB_ACCESS_TOKEN:}
github.repo=hclds-keycloak
github.branch=main
github.owner=HCL-TECH-SOFTWARE


# Debug logging to print requests
logging.level.dev.langchain4j=DEBUG
logging.level.dev.ai4j.openai4j=DEBUG
langchain4j.open-ai.streaming-chat-model.log-requests=true
langchain4j.open-ai.streaming-chat-model.log-responses=true